iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
library(ggvis)
install.packages("ggvis")
library(ggvis)
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)
iris
head(iris)
str(iris)
summary(iris[c("Petal.Width", "Sepal.Width")])
library(class)
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))
summary(iris_norm)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
iris.training <- iris[ind==1, 1:4]
iris.test <- iris[ind==2, 1:4]
iris.trainLabels <- iris[ind==1, 5]
iris.testLabels <- iris[ind==2, 5]
iris_pred <- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)
iris_pred
iris_pred %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
library(gmodels)
install.packages("gmodels")
install.packages("gmodels")
library(gmodels)
CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
data.frame()
data.frame(wbcd)
wbcd <- wbcd[-1]
wbcd
table(wbcd$diagnosis)
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
table(wbcd$diagnosis)
wbcd$diagnosis<- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
normalize(c(1, 2, 3, 4, 5))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis<- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
install.packages("class")
install.packages("class")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
install.packages("class")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
library("class", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
wbcd
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis<- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis<- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
table(wbcd$diagnosis)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
library("gmodels", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
install.packages("ROCR")
library("ROCR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
pred <- prediction(predictions = wbcd_test_labels,
labels = wbcd_test_pred)
pred <- prediction(predictions = wbcd_test_labels,labels = wbcd_test_pred)
pred <- prediction(predictions = wbcd_test_pred,labels = wbcd_test_labels)
eval(teste)
teste <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
eval(teste)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
teste <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
eval(wbcd_test_pred)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
train <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
library("MASS", lib.loc="/usr/lib/R/library")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
train <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
train <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
data(fgl,package="MASS")
grp=fgl$type
X=scale(fgl[,1:9])
k=length(unique(grp))
dat=data.frame(grp,X)
n=nrow(X)
ntrain=round(n*2/3)
require(class)
set.seed(123)
train=sample(1:n,ntrain)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
install.packages("chemometrics")
grp=fgl$type
X=scale(fgl[,1:9])
k=length(unique(grp))
dat=data.frame(grp,X)
n=nrow(X)
ntrain=round(n*2/3)
require(class)
set.seed(123)
train=sample(1:n,ntrain)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
library("chemometrics", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
set.seed(123)
train=sample(1:n,ntrain)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
train <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
ntrain <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
train=sample(1:n,ntrain)
ntrain <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
ntrain
train=sample(1:n,ntrain)
ntrain <- CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd
attach(wbcd)
names(wbcd)
cor(texture_mean, compactness_worst)
regressao <- (texture_mean, compactness_worst)
regressao <- (texture_mean, compactness_worst)
regressao <- (texture_mean~compactness_worst)
regressao
data(fgl,package="MASS")
grp=fgl$type
X=scale(fgl[,1:9])
k=length(unique(grp))
dat=data.frame(grp,X)
n=nrow(X)
ntrain=round(n*2/3)
require(class)
set.seed(123)
train=sample(1:n,ntrain)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
data(fgl,package="MASS")
grp=fgl$type
X=scale(fgl[,1:9])
k=length(unique(grp))
dat=data.frame(grp,X)
n=nrow(X)
ntrain=round(n*2/3)
require(class)
set.seed(123)
train=sample(1:n,ntrain)
resknn=knnEval(X,grp,train,knnvec=seq(1,30,by=1),legpos="bottomright")
title("kNN classification")
da <- read.table("http://www.leg.ufpr.br/~walmes/cursoR/anovareg.txt",
header=TRUE, sep="\t")
str(da)
x <- ordered(seq(1,10,1))                     # seqüência regular
y <- ordered(c(1,1.5,2.5,3,3.5,4,4.5,6,8,10)) # seqüência irregular
X <- model.matrix(~x)    # matriz do modelo para níveis de x
Y <- model.matrix(~y)    # matriz do modelo para níveis de y
cbind(X[,2:3], Y[,2:3])  # observe que as matrizes são iguais
round(t(X)%*%X)          # colunas ortogonais implica em matriz de covariância
unique(da$dose)           # doses únicas usadas
da$ds <- ordered(da$dose) # fator ordenado
str(da$ds)                # mostra a ordem dos níveis
m0 <- lm(indice~bloco+cultivar*ds, data=da)
par(mfrow=c(2,2)); plot(m0); layout(1) # checa as pressuposições
m1 <- lm(indice~bloco+cultivar*dose, data=da)
par(mfrow=c(2,2)); plot(m1); layout(1) # gráfico 1,1 aponta que requer termos
anova(m0, m1)                          # compara modelos aninhados, testa lof
m2.1 <- lm(indice~bloco+cultivar*(dose+I(dose^2)), data=da)
m2.2 <- lm(indice~bloco+cultivar*poly(dose, degree=2), data=da)
m2.3 <- lm(indice~bloco+cultivar*poly(dose, degree=2, raw=TRUE), data=da)
sapply(list(m2.1, m2.2, m2.3), coef)   # coeficientes lado a lado
sapply(list(m2.1, m2.2, m2.3), fitted) # valores ajustados lado a lado
m2 <- m2.1
par(mfrow=c(2,2)); plot(m2); layout(1) # ok
anova(m0, m2)                          # modelo quadrático não apresenta lof
anova(m2)                              # teste de hipóteses com o modelo final
of <- cbind(o=da$indice, f=fitted(m2)); of
plot(of)
bl <- levels(da$bloco)[1] # nível de bloco usando na predição
ct <- levels(da$cultivar) # todos os níveis de cultivar serão usados
ds <- seq(0,300,by=2)     # seqüência fina de valores para dose
pred <- expand.grid(bloco=bl, cultivar=ct, dose=ds) # data.frame da predição
aux <- predict(m2, new=pred, interval="confidence") # IC 95%
str(aux) # matriz com colunas fit, lwr, upr
pred1 <- cbind(pred, as.data.frame(aux))
pred <- expand.grid(bloco=bl, cultivar=ct, dose=ds) # data.frame da predição
aux <- predict(m2, new=pred, interval="confidence") # IC 95%
str(aux) # matriz com colunas fit, lwr, upr
pred1 <- cbind(pred, as.data.frame(aux))
str(pred1) # contém apenas o nível I de bloco
require(reshape)
install.packages("reshape")
require(reshape)
pred1 <- melt(pred1, id=c("cultivar","dose","bloco"))
names(pred1)[ncol(pred1)] <- "indice"
str(pred1)
da$variable <- "obs"
str(da)
pred1 <- rbind(da[,c("cultivar","dose","bloco","variable","indice")], pred1)
str(pred1) # junção dos valores observados e preditos
require(lattice)
xyplot(indice~dose|cultivar, groups=variable, data=pred1,
distribute.type=TRUE, sub="predição para o bloco I",
type=c("l","l","p","l"), col=c(2,3,1,3), lty=c(1,2,0,2))
m3 <- lm(formula(m2), data=da, contrasts=list(bloco=contr.sum))
cbind(coef(m2), coef(m3)) # os coeficientes de bloco mudaram
blI <- coef(m3)["bloco1"]; blI # efeito do bloco I sob a restrição soma zero
pred1[pred1$variable!="obs",]$indice <- pred1[pred1$variable!="obs",]$indice-blI
xyplot(indice~dose|cultivar, groups=variable, data=pred1,
distribute.type=TRUE, sub="valores preditos sem efeito de bloco",
type=c("l","l","p","l"), col=c(2,3,1,3), lty=c(1,2,0,2))
X <- model.matrix(~cultivar*(dose+I(dose^2)), data=pred)
b <- coef(m3)[-grep("bloco", names(coef(m3)))]
se <- predict(m3, newdata=pred, se.fit=TRUE)$se
tc <- qt(0.975, df=df.residual(m3))
me <- se*tc
pred2 <- pred[,-1]
pred2$fit <- X%*%b
pred2$lwr <- pred2$fit-me; pred2$upr <- pred2$fit+me
pred2 <- melt(pred2, id=c("cultivar","dose"))
names(pred2)[ncol(pred2)] <- "indice"
pred2 <- rbind(da[,c("cultivar","dose","variable","indice")], pred2)
str(pred2) # data.frame dos valores preditos pelo método matricial
xyplot(indice~dose|cultivar, groups=variable, data=pred2,
distribute.type=TRUE, layout=c(3,1),
type=c("l","l","p","l"), col=c(2,3,1,3), lty=c(1,2,0,2),
strip=strip.custom(bg="gray90"),
xlab="Dose de nitrogênio (kg/ha)", ylab="Índice agronômico",
sub=list("predição livre do efeito de bloco", cex=0.8, font=3),
key=list(space="top", columns=3, type="o", divide=1,
lines=list(pch=c(1,NA,NA), lty=c(0,1,2), col=c(1,2,3)),
text=list(c("valores observados","valores preditos","IC 95%"))))
round(pred1$indice-pred2$indice, 3) # pred1 e pred2 são iguais
install.packages("dplyr")
install.packages("tidyr")
install.packages("ggplot2")
source('~/.active-rstudio-document')
idade <- c(10, 22, 30, 5, 7)
nome <- c("danilo", "layse", "cida", "")
tabela <- data.frame(nome, idade)
cor(texture_mean, compactness_worst)
source('~/R/workspace/aprendendoML/knnFinal.R')
source('~/R/workspace/aprendendoML/knnFinal.R')
source('~/R/workspace/aprendendoML/knnFinal.R')
source('~/R/workspace/aprendendoML/knnFinal.R')
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
source('~/R/workspace/aprendendoML/knnFinal.R', echo=TRUE)
idade <- c(10, 22, 30, 5, 7)
nome <- c("danilo", "layse", "cida", "josivaldo", "juliana")
tabela <- data.frame(nome, idade)
tabela
View(tabela)
nome <- c("danilo", "layse", "cida", "josivaldo", "juliana")
tabela <- data.frame(nome, idade)
tabela
install.packages("e1071")
library(e1071)
classification <- function() {
x <- c(rnorm(50, mean=0), rnorm(50, mean=4))
y <- c(rnorm(50, mean=4), rnorm(50, mean=0))
z <- c(rep("1", 50), rep("0", 50))
data <- data.frame(x, y, z)
}
data <- classification()
training_set = sample(100,67)
train <- data[training_set,]
test <- data[(1:100)[-training_set],]
model <- naiveBayes(z~., data=train)
model$apriori
model$tables
predictions <- predict(model, test[,1:2])
table(predictions, test$z)
classification <- function() {
x <- c(rnorm(50, mean=0), rnorm(50, mean=4))
y <- c(rnorm(50, mean=4), rnorm(50, mean=0))
data <- data.frame(x, y)
}
data <- classification()
model <- kmeans(data, 2)
summary(model)
table(model$cluster)
plot(data)
points(model$centers, pch=19)
classification
classification
classification
classification <- function() {
x <- c(rnorm(50, mean=0), rnorm(50, mean=4))
y <- c(rnorm(50, mean=4), rnorm(50, mean=0))
data <- data.frame(x, y)
}
wbcd <- read.csv("/home/danilo/R/workspace/aprendendoML/wisc_bc_data.csv", stringsAsFactors = FALSE)
data <- wbcd
model <- kmeans(data, 2)
data <- wbcd
model <- kmeans(data, 6)
library(caret)
install.packages(caret)
install.packages('caret')
library(caret)
data(longley)
fit <- knnreg(longley[,1:6], longley[,7], k=3)
summary(fit)
predictions <- predict(fit, longley[,1:6])
mse <- mean((longley$Employed - predictions)^2)
print(mse)
summary(fit)
predictions <- predict(fit, longley[,1:6], metric="Accuracy")
mse <- mean((longley$Employed - predictions)^2)
print(mse)
library(caret)
library(mlbench)
install.packages("mlbench")
library(mlbench)
data(PimaIndiansDiabetes)
control <- trainControl(method="cv", number=5)
set.seed(7)
fit <- train(diabetes~., data=PimaIndiansDiabetes, method="glm", metric="Accuracy", trControl=control)
print(fit)
control <- trainControl(method="cv", number=5)
set.seed(7)
fit <- train(diabetes~., data=PimaIndiansDiabetes, method="glm", metric="Recall", trControl=control)
print(fit)
library(caret)
library(mlbench)
data(PimaIndiansDiabetes)
control <- trainControl(method="cv", number=5)
set.seed(7)
fit <- train(diabetes~., data=PimaIndiansDiabetes, method="glm", metric="Recall", trControl=control)
library(caret)
library(mlbench)
data(PimaIndiansDiabetes)
control <- trainControl(method="cv", number=5)
set.seed(7)
fit <- train(diabetes~., data=PimaIndiansDiabetes, method="glm", metric="Precision", trControl=control)
library(caret)
data(iris)
fit <- knn3(Species~., data=iris, k=5)
summary(fit)
predictions <- predict(fit, iris[,1:4], type="class")
table(predictions, iris$Species)
predictions <- predict(fit, iris[,1:4], type="class")
print(predictions)
library(caret)
data(iris)
fit <- knn3(Species~., data=iris, k=5)
summary(fit)
predictions <- predict(fit, iris[,1:4], type="class", method='Accuracy')
print(predictions)
table(predictions, iris$Species)
install.packages("RWeka")
library(RWeka)
install.packages("RWeka")
install.packages("RWeka")
install.packages("RWeka", lib="/path/to/library")
install.packages("RWekajars")
library(RWeka)
install.packages("RWeka")
library(RWeka)
install.packages("RWeka")
install.packages('caret')
library(caret)
library(caret)
data(longley)
hist(longley)
install.packages('caret')
library(caret)
data(longley)
longley
data(longley$Year)
library(caret)
data(longley$Year)
data(longley)
hist(longley$Year)
boxplot(longley$Year)
plot(longley$Year)
setwd("/home/danilo/R/workspace/atividadeML/")
if(!exists("foo", mode="function")) source("knn_final.R")
if(!exists("foo", mode="function")) source("bayes_final.R")
if(!exists("foo", mode="function")) source("c5_0.R")
listValuesKnn <- knnFunction(getwd())
listValuesBayes <- bayesFunction(getwd())
listValuesC5_0 <- c5_0Function(getwd())
listValuesKnn
listValuesBayes
listValuesC5_0
listPrecision <- c(listValuesKnn[1], listValuesBayes[1], listValuesC5_0[1])
listRecall <- c(listValuesKnn[2], listValuesBayes[2],listValuesC5_0[2])
listFMeasure <- c(listValuesKnn[3], listValuesBayes[3], listValuesC5_0[3])
plot(listPrecision)
plot(listRecall)
plot(listFMeasure)
